---
title: "Maching learning and dumbbell exercises"
output: html_document
---

**Introduction**

Human Activity Recognition (HAR) is a means of monitoring human activity, through mobile devices attached to the body.  Different types of body movement can be measured, and it is then possible to categorize a series of body movements as a particular action.  For example in one study (Anguita et al. 2012) participants wore a Samsung smart phone, and its built in accelerometer and gyroscope took a range of readings.  On the basis of these readings, it was possible to categorize, with a high degree of accuracy, whether at any particular time a participant was lying down, walking, walking upstairs, walking downstairs, sitting or standing.

Another approach to HAR is to focus on optimum body movements.  This can be important in terms of sport, exercise, and other activities that put a high premium on motor skills.  One study (Velloso et al., 2013) focused on weight lifting.  Participants performed a weight lifting exercise in five different ways, and only one of the five ways was defined as correct.  By recording the bodyâ€™s movements and position at the time of the exercise, one might be able to match these movements and positions with the correct performance of the exercise, which could be useful in terms of coaching.

This paper has the Velloso study as its topic.  It will report on an attempt to analyze the data from the study, using machine learning algorithms, to find out the extent to which the five different ways can be identified.

**Methodology**

The raw data from the Velloso study consisted of 19622 rows of information, having 160 different variables.  The data was generated by six participants.  The six participants' relative contributions were as follows:

```{r, echo=FALSE}
setwd("C:/")
dataset <- read.csv("pml-training.csv")
table(dataset$user_name)
```

One of the variables, "classe", was for categorization into the five activities, labelled "A" through to "E", and it was therefore the dependent variable.  These were the frequencies for this variable:

```{r, echo=FALSE}
table(dataset$classe)
```
A number of variables were removed from the analysis.  Variables for the participants names, as well as time stamps, were removed.  Two variables called "new_window" and "num_window" were also removed, because they did not seem to be related to body movements.  

There were a number of variables which had missing data, in the form of NAs.  A scattergun approach to removal of these variables was taken, where any variable classed as a factor, or which had an "NA" in its first row, was removed, using this function:
```{r}
remove <- function(x){
  if(is.na(dataset[1,x])){
    dataset[,x] <<- NULL
  }
  if(is.factor(dataset[,x])){
  dataset[,x] <<- NULL
  } 

}
```
```{r, echo=FALSE}


dataset[,1] <- NULL
dataset[,2] <- NULL
dataset[,3] <- NULL
dataset[,4] <- NULL
dataset[,5] <- NULL
dataset[,6] <- NULL
dataset[,7] <- NULL
dataset$raw_timestamp_part_2 <- NULL
source("C:/remove.R")
len <- dim(dataset)[2]-1
s <- sapply(len:1,remove)

```
This reduced the number of variables to 50, plus the dependent variable.  The remaining variables were:
```{r echo=FALSE}
names(dataset)
```
Having cleaned the dataset, it was divided into a training and testing set, using commands from R's caret package.

The training set can be treated as a cross-validation set.  An additional 20 cases were provided, aside from the main dataset, for the purpose of testing.  However the cross-validation set will be described as a testing set.

Seventy percent of the dataset were randomly allocated to the training set, 30% to the test set (for cross-validation), using the caret package's createDataPartition command:

```{r}
library(caret)
```
```{r}
set.seed(100)
inProject <- createDataPartition(y=dataset$classe,p=0.7,list=FALSE)
training <- dataset[inProject,]
testing <- dataset[-inProject,]
#Number of cases in the training set:
dim(training)[1]
#Number of cases in the testing set:
dim(testing)[1]

```
Once the training and testing sets had been created, two models were created and validated.  One used trees, the other random forests.

**Results**

The training set was validated using a trees approach to maching learning, specifically the "rpart" method in caret. The dependent variable was the category of exercise, and there were 50 independent variables.  
```{r}
modelTrees <- train(classe~.,method="rpart",data=training)
```
On the basis of this model, Gini scores were calculated, to highlight the variables with the most importance, in terms of predicting the five categories of exerices.
```{r echo=FALSE}
varImp(modelTrees)
```
It can be seen that thirteen of the fifty independent variables have Gini scores greater than zero, with "pitch_forearm" being the highest.

Using the rattle package, one can create a diagram to show the decisions that the tree model made, in selecting the most likely activity from the training data.
```{r echo=FALSE}
library(rattle)
fancyRpartPlot(modelTrees$finalModel)
```

The tree diagram clearly shows that there are problems with the model.  There are five cateogries, A through to E, yet the model was unable to come up with a condition in which category D would be chosen.  This is in spite of the fact that in the raw data over 16% of the 19622 rows were categorized by this letter.  It was therefore decided to try a random forest approach, to see if there was any improvement.

The randomForest package was used, and all independent variables were entered.
```{r}
library(randomForest)
set.seed(100)
modelRf <- randomForest(classe~.,data=training)
```
The random forest model would appear to have a high degree of accuracy:
```{r echo=FALSE}
print(modelRf)
```
The in sample error rate was 0.67%, and the model had no difficulty picking up category D dumbbell routines.  Looking at the Gini scores, the most important variable is roll_belt, followed by yaw_belt.  This contrasts with the trees model, which had pitch_forearm and magnet_dumbbell_y as having the highest impact.

```{r echo=FALSE}
varImp(modelRf )
```
Given the in sample error rate of 0.67%, one would expect the out of sample error rate to be higher, but still to be very small - perhaps between 1 and 3 percent.  The model was compared with the validation set, which had not been used to create this model:
```{r echo=FALSE}
pred <- predict(modelRf,testing)
testing$predict <- pred
testing$correct <- testing$predict == testing$classe
#numbers correctly (TRUE) and incorrectly (FALSE) classified
table(testing$correct)
```
5851 results were correct, and 34 were not correct.  This gives an overall accuracy of 0.58%, meaning that the out of sample error rate was actually less than the in sample.  The full confusion matrix for the cross-validation is as follows:
```{r echo=FALSE}
table(testing$classe,testing$predict)
```
**Conclusion**

This paper reported on an attempt to match the readings from a device attached to weightlifters' bodies with particular types of dumbbell exercise.  The type of exercise was the dependent variable, and there were potentially 159 independent variables.  The independent variables were reduced to 50, using a fairly arbitrary approach.  Some of the variables removed may have been good predictors of the dependent variable, but it was assumed that with so many variables, this loss would have been compensanted for amongst the 50 remaining variables.

The first model attempted was trees.  This was chosen because of its relative simplicity, compared with random forests, and the ease with which the results could be interpreted - for example with the tree diagram.  Unfortunatly this model failed to identify category D, in sample, when over 16% of the complete dataset had this category.

The second model tried, the random forest model, was successful.  The in sample error rate was 0.67% and the out of sample error rate even less, at 0.58%.  It would appear that for HAR projects like the one described in this paper, a random forest approach to analyzing the data can be very effective.

**References**

Anguita, D., Ghio, A., Oneto, L., Parra, X.& Reyes-Ortiz, J.L. (2012). Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living. Vitoria-Gasteiz, Spain.

Velloso, E., Bulling, A., Gellersen, H., Ugulino, W.& Fuks, H. (2013). Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI.